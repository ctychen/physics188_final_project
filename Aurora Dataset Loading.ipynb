{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f938a12-254f-4303-b52b-8b49215259f2",
   "metadata": {},
   "source": [
    "This code is heavily based on the code provided to us by Dr. Harding, Alex Toohey, Tommy Duong, and Sabrina Nazarzai on a research project using the pyaurorax package. The part of the code that downloads the data is essentially the same, with a few changes to variables and flow. The code for the multiple day downloading was also based on their code with changes to the logic and the specified parameters to allow for more control over data downloading. The saving of the frames as pdfs is original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2fec5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pyaurorax as auro\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(start_time,end_time,site_id,cleanup=True):\n",
    "    aurorax = auro.PyAuroraX() #creating class instance\n",
    "    dataset_name = \"TREX_RGB_RAW_NOMINAL\" #name of dataset that will be used for extracting data\n",
    "    download = aurorax.data.ucalgary.download(dataset_name=dataset_name,start=start_time,end=end_time,site_uid=site_id) #downloads the data, takes a while\n",
    "\n",
    "    frames = [] #don't know the number of elements and frames is in a shape that is pre-assigned, making list appending easier\n",
    "\n",
    "    for filename in download.filenames: #loops over each image downloaded\n",
    "        with h5py.File(filename,'r') as f: #uses h5py file for efficient data storage and extraction\n",
    "            images = f['data/images'][:].transpose(3,0,1,2) #transposes the matrix so it is in shape [N,H,W,C] where N is frame number, H is height, \n",
    "            #W is width of image, and C is the RGB color chanel (0-2)\n",
    "            timestamp_data = f['data/timestamp'][:] #extracts the time the image was taken\n",
    "            times = [datetime.fromisoformat(t.decode('utf-8').replace(' UTC','')).replace(tzinfo=None,microsecond=0) for t in timestamp_data]\n",
    "            #the above code takes a byte object from the h5py file decodes it into a string, replaces the timezone, makes it into a datetime object,\n",
    "            #then ensures that object is timezone-naive and drops the fractional seconds\n",
    "\n",
    "            #sampling frames so that we don't have too much data as most of the images will be the same. \n",
    "            last_minute = None\n",
    "            for i,t in enumerate(times): #loops over the times\n",
    "                if (t.hour,t.minute) != last_minute: #filters out data from the same minute, captures 1 frame per minute\n",
    "                    frames.append(images[i])\n",
    "                    last_minute = (t.hour,t.minute)\n",
    "\n",
    "    #to delete the repository after samples frames from the original \n",
    "    if cleanup == True and download.filenames:\n",
    "    # Find the pyaurorax_data root directory\n",
    "        first_file = str(download.filenames[0])  # Convert to string\n",
    "        # Navigate up to find pyaurorax_data\n",
    "        path_parts = first_file.split(os.sep)\n",
    "        if 'pyaurorax_data' in path_parts:\n",
    "            idx = path_parts.index('pyaurorax_data')\n",
    "            root_dir = os.sep.join(path_parts[:idx+1])\n",
    "            try:\n",
    "                shutil.rmtree(root_dir) #removes directory\n",
    "                print(f\"Removed download directory: {root_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not remove directory {root_dir}: {e}\")\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4e8aad-94c1-4968-9a5b-70a2be1c4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"start = datetime(2024, 1, 15, 0, 0, 0)\n",
    "end = datetime(2024, 1, 15, 23, 59, 59)\n",
    "site_uid = \"gill\" \n",
    "frames, timestamps, locations = download_data(start, end, site_uid)\n",
    "\n",
    "print(frames.shape)\"\"\"\n",
    "#testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca87eda-67a8-4561-ac41-6cd071862726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_download(start_date, end_date, site, start_hour = 0, end_hour=4):\n",
    "    #actually runs the download for multiple days if necessary, has the time preset to the time when auroras are usually expected.\n",
    "    #note: start_date, end_date are datetime objects, start_hour, end_hour are ints, site is a string.\n",
    "    \n",
    "    all_frames = [] #again, not sure how large the array or list needs to be, so using empty list\n",
    "    all_sites = []\n",
    "    \n",
    "    if type(site) == str:\n",
    "        site = [site]\n",
    "\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date: #loop over days\n",
    "        day_start = current_date.replace(hour=start_hour, minute=0, second=0) #using a datetime object and leaving the year, month, day unchanged\n",
    "        day_end = current_date.replace(hour=end_hour, minute=59, second=59)\n",
    "        \n",
    "        for loc in site: #loops over multiple sites\n",
    "            frames = download_data(day_start,day_end,site_id=loc,cleanup=True) #makes the frames\n",
    "            n = frames.shape[0]\n",
    "            all_frames.append(frames)\n",
    "            all_sites.extend([loc]*n)\n",
    "\n",
    "        current_date += timedelta(days=1) #increment the current date by 1 using timedelta\n",
    "\n",
    "    return all_frames, np.array(all_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0ce8de-2104-4c10-9d54-1c8c7bba7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames, sites,dir_name = \"observation_images\"):\n",
    "    #function saves the frames \n",
    "    out_dir = dir_name\n",
    "    os.makedirs(out_dir,exist_ok=True) #makes a new directory, doesn't crash if the directory already exists\n",
    "    for i in range(frames.shape[0]):\n",
    "        plt.imsave(os.path.join(out_dir, f\"{sites[i]}_frame_{i:07d}.png\"),frames[i]) #saves the images as png for classification\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5a4f7-9129-479e-b1f8-293eef6fb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test cell\n",
    "#note datetime follow this pattern:\n",
    "#(xxxx, xx, xx, xx, xx, xx)\n",
    "#(year, month, day, hour, minute, second)\n",
    "#all arguments are not necessary\n",
    "# may 10-13 2024 geomagnetic storm as baseline \n",
    "start_date = datetime(2024,5,10)\n",
    "end_date = datetime(2024,5,11)\n",
    "start_hour = 0 \n",
    "end_hour=4\n",
    "# site = [\"gill\",\"gill\",\"atha\"]\n",
    "# site = [\"fsmi\",\"luck\",\"pina\",\"rabb\",\"yknf\",\"gill\",\"atha\"]\n",
    "site = [\"luck\",\"pina\"]\n",
    "\n",
    "all_frames, all_loc = run_download(start_date=start_date,end_date=end_date,site=site, start_hour=start_hour, end_hour=end_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_frames[0]), len(all_frames[1]), len(all_loc), all_frames[0].shape, all_frames[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6367a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn all_frames into numpy array \n",
    "list_of_frames = []\n",
    "for frames in all_frames:\n",
    "    list_of_frames.extend(frames)\n",
    "list_of_frames = np.array(list_of_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be556eeb-71a8-49dd-b4fd-3d6998d27217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_frames(all_frames, all_loc)\n",
    "save_frames(list_of_frames, all_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = auro.PyAuroraX()\n",
    "print(test.data.ucalgary.list_datasets())\n",
    "obs = test.data.ucalgary.list_observatories('trex_rgb')\n",
    "for i in range(len(obs)):\n",
    "    print(obs[i].uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9db00-e6f0-4d7c-b683-0a8f2b3ba890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
